## csp-adapter-rancher-e2e-automation

CSP Billing Adapter Rancher End-to-End Automation

This repo contains deployment and test scripts for testing the csp-billing-adapter for rancher in AWS cloud service provider.


### Terraform for deployment

Terraform which creates the [pre-requisites](https://ranchermanager.docs.rancher.com/integrations-in-rancher/cloud-marketplace/aws-cloud-marketplace/adapter-requirements) for the csp adapter install. 

Please see the README.md in the terraform folder for additional instructions to deploy EKS cluster on AWS, with rancher, csp-rancher-usage-operator, csp-billing-adapter installed on this cluster.
Once deployment is completed successfully, you can move on to the tests section.

Note: Currently, installation of the managed licensed offer of csp-adapter is not supported yet although the pre-requistes are created. This is planned for the next iteration of changes.

### Tests

These tests make use of the rancher test [framework](https://github.com/rancher/rancher/tree/release/v2.7/tests/framework) 
for testing the billing feature. Rancher test framework is written in go. 

Go must be installed on the local machine where you are runnig the tests.

After cloning this repo, run ```go mod download```

To run the billing tests, you'll need to set an environment variable: CATTLE_TEST_CONFIG (path to config file).

NOTE:
The terraform deployment scripts *automatically* create the cattle-config-e2e.yaml with the contents specific to the environment deployed.

The user does not have to change this yaml file unless the user is targetting to run these tests against a different rancher deployment other than the one deployed by terraform.

The contents of this yaml file generated by terraform scripts look like this:

```
rancher:
  host: <rancher host>
  adminToken: <token>
  cleanup: true
awsCredentials:
  secretKey: <your aws secret key>
  accessKey: <your aws access key>
  defaultRegion: us-west-2
eksClusterConfig:
  imported: false
  kmsKey: ""
  kubernetesVersion: "1.24"
  loggingTypes: []
  nodeGroups:
  - desiredSize: 3
    diskSize: 20
    ec2SshKey: ""
    gpu: false
    imageId: ""
    instanceType: t3.medium
    labels: {}
    maxSize: 3
    minSize: 2
    nodegroupName: "node group name"
    requestSpotInstances: false
    resourceTags: {}
    spotInstanceTypes: []
    subnets: []
    tags: {}
    userData: ""
    version: "1.24"
  privateAccess: false
  publicAccess: true
  publicAccessSources: []
  region: us-west-2
  secretsEncryption: false
  securityGroups:
  - ""
  serviceRole: ""
  subnets:
  - subnet-0a28179d7f5d6e105
  - subnet-0cff6bc389a62d584
  tags: {}

```


You can point the CATTLE_TEST_CONFIG environment variable at the file created by the terraform scripts.

```bash
export CATTLE_TEST_CONFIG=<repopath>/terraform/cattle-config-e2e.yaml
```

The automated tests in this repo can be run on the command-line, example:

```
/usr/local/go/bin/go test -timeout 30m -run ^TestBillingPAYGTestSuite$ csp-adapter-rancher-e2e-automation/tests
```

You can also run it in verbose mode with the -v option.
Example:

```
/usr/local/go/bin/go test -timeout 30m -run ^TestBillingPAYGTestSuite$ csp-adapter-rancher-e2e-automation/tests -v
=== RUN   TestBillingPAYGTestSuite
=== RUN   TestBillingPAYGTestSuite/TestValidateCSPBillingAdapterPodStatus
=== RUN   TestBillingPAYGTestSuite/TestValidateCSPRancherUsageOperatorPodStatus
=== RUN   TestBillingPAYGTestSuite/TestValidateTotalCountAfterProvisioningHostedEKSCluster
time="2023-06-27T21:59:50+02:00" level=info msg="Dynamic Client Host:a1f0e3f357a2b4d41b040caed8387622-384024445.us-west-2.elb.amazonaws.com"
time="2023-06-27T22:13:48+02:00" level=info msg="Cluster status is active!"
time="2023-06-27T22:13:49+02:00" level=info msg="serviceAccountTokenSecret in this cluster is: cluster-serviceaccounttoken-8nv8n"
time="2023-06-27T22:13:49+02:00" level=info msg="All nodes in the cluster are in an active state!"
time="2023-06-27T22:13:51+02:00" level=info msg="Pod cattle-cluster-agent-6d98c5447d-nv9l8: Active | Image: docker.io/rancher/rancher-agent:v2.7.4"
time="2023-06-27T22:13:51+02:00" level=info msg="Pod aws-node-gdb2f: Active | Image: 602401143452.dkr.ecr-fips.us-east-1.amazonaws.com/amazon-k8s-cni:v1.11.4"
time="2023-06-27T22:13:51+02:00" level=info msg="Pod aws-node-vtfsp: Active | Image: 602401143452.dkr.ecr-fips.us-east-1.amazonaws.com/amazon-k8s-cni:v1.11.4"
time="2023-06-27T22:13:51+02:00" level=info msg="Pod coredns-799c5565b4-7dlx2: Active | Image: 602401143452.dkr.ecr.us-west-2.amazonaws.com/eks/coredns:v1.8.7-eksbuild.3"
time="2023-06-27T22:13:51+02:00" level=info msg="Pod coredns-799c5565b4-lh2ms: Active | Image: 602401143452.dkr.ecr.us-west-2.amazonaws.com/eks/coredns:v1.8.7-eksbuild.3"
time="2023-06-27T22:13:51+02:00" level=info msg="Pod kube-proxy-km6l7: Active | Image: 602401143452.dkr.ecr-fips.us-east-1.amazonaws.com/eks/kube-proxy:v1.24.7-minimal-eksbuild.2"
time="2023-06-27T22:13:51+02:00" level=info msg="Pod kube-proxy-vzdx4: Active | Image: 602401143452.dkr.ecr-fips.us-east-1.amazonaws.com/eks/kube-proxy:v1.24.7-minimal-eksbuild.2"
time="2023-06-27T22:13:51+02:00" level=info msg="Dynamic Client Host:a1f0e3f357a2b4d41b040caed8387622-384024445.us-west-2.elb.amazonaws.com"
time="2023-06-27T22:13:53+02:00" level=info msg="Dynamic Client Host:a1f0e3f357a2b4d41b040caed8387622-384024445.us-west-2.elb.amazonaws.com"
--- PASS: TestBillingPAYGTestSuite (847.28s)
    --- PASS: TestBillingPAYGTestSuite/TestValidateCSPBillingAdapterPodStatus (0.77s)
    --- PASS: TestBillingPAYGTestSuite/TestValidateCSPRancherUsageOperatorPodStatus (0.50s)
    --- PASS: TestBillingPAYGTestSuite/TestValidateTotalCountAfterProvisioningHostedEKSCluster (844.86s)
PASS
ok  	csp-adapter-rancher-e2e-automation/tests	847.338s
```

The provisioning test in the testsuite is written to clean up any resources it created in AWS. This cleanup in the background takes a few minutes (give it close to 10 minutes). However, please makes sure to login into AWS console and check that resources are indeed deleted so we don't incur additional cost. The provisioning test creates a cluster with the name "auto-ekshostclusterb-*****" and in order to delete this if not cleaned up automatically, first delete the node group first which can be accessed in the Compute tab of the cluster detail and once the node group is deleted you can proceed to delete the cluster.